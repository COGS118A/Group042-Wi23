{
	"cells": [{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# COGS 118A - Project Checkpoint"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Names\n",
				"\n",
				"- Masiah Manzano\n",
				"- Jesus Enrique Mendez\n",
				"- Armaan Johal\n",
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Abstract \n",
				"The goal that we want to solve is to help the world find an answer to whatever it is that they are asking for. We thought back to how when we were young, we could ask a magic 8 ball anything and receive an answer, but what if we can somehow manipulate this so that the magic 8 ball can be more like how CHAT GPT is for everyone nowadays. Where the user will ask a question they feel they need an answer to that is not necessarily academic, and unlike CHAT GPT, it can be about anything with no restrictions and still receive an answer.The data we will be utilizing is yet to be determined but we plan on taking a public poll from our peers on what questions they would really want answered and it can be about anything. Lastly, success will be measured when an adequate answer can be given that means satisfactory for the one who is asking for magic 8 ball. The data used throughout this project represents what the user will be inputting in the format of a question and it will be measured through various responses that are placed into the code to give the user a variety of different answers for what they are asking. Lastly, performance will be measured if the answers given back actually make sense for what is asked, if it is too random then this will be measured as a failure since our goal here is to give back the user a answer they can be happy with. \n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Background\n",
				"\n",
				"What helped us come up with this idea is finding ways to make a lot of input data be used and give back an output. This is important because as we researched online, we found that many machine learning algorithms receive a lot of data and because of this cannot scale as good as it needs to. A good example of this is when algorithms use K-means. A paper we found (https://dl.acm.org/doi/10.1145/2347583.2347596) discusses how K-means is a learning algorithm that implements this and as the data contains to increase, the time it takes to output is excessive. So what if we can use the 8 ball simulator as a way to limit down response times even if the data is huge (this general problem is still likely to change but this is the best we could come up with to make sure we can still use this idea as a ML-learning solution for a ML problem). Another reference that dives deeper into scalability comes from (https://livebook.manning.com/book/real-world-machine-learning/chapter-9/1) Here we see how scalability has been a constant issue for many years now for algorithms and we are trying to find ways to use our idea to combat against this or to showcase how we can better the workload while still using a lot of data sets"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Problem Statement\n",
				"\n",
				"Given the feedback we received, we decided to take a different approach on how we plan to use what we came up with. We still want to use our 8 ball simulator but in a way that can help answer a ML-potential question and while researching certain ways to better this problem, we have come to the conclusion that we should take an approach of either scalability, or to help combat biased algorithms and systems. We believe that our current 8 ball simulator can either focus primarily on both or one of these because, if we take the approach to help scalability since most models become difficult as data is increased, we want to showcase that this is not always the case because we want to be able to generate as many answers as we possibly can and still be able to give an output. Again, this is still a work in progress but the general area we want to focus on is scalability and how we can use this simulation as an example for other models who suffer from too much data, which this can be used as an example how they can better their models to take up a lot of data and still work good."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Data\n",
				"\n",
				"- Since it would take a really long time we decided it is best to not interview others (as fun as it may be) and instead just search on our own time commonly asked questions and gather information all in just one sitting and apply this to our code. We decided to research on some common questions asked by people and we figured the best way to do this is by asking questions that truly get to know someone so we stumbled upon these 100 questions and here is the link https://www.signupgenius.com/groups/getting-to-know-you-questions.cfm. We primarily used this as a reference for the type of questions we want to ask and for what to expect to receive if these types of questions are asked. Another example that we took a look at to help combat scalability issues with our code and to help better other algorithms using this idea of an 8 ball simulator, we took a look at Ray. Ray is a software program that takes heavy workloads of code and makes it really easy to scale the workload. We are going to continue looking at how this program seems to help other algorithms and see if we can apply this or any of the methods used to better our general idea on how to tackle a ML-problem \n",
				"- As of right now our number of current variables are not as high as we would like but we are still constantly updating this so that it can reach a great amount for users to ask as many questions as they want and not receive similar answers to said questions\n",
				"- Our observations consist of finding more ways for us to implement methods of greater responses. What we mean by this is that we want to be able to answer questions without a one word answer. We want the user to actually feel as if they are having a conversation with our code\n",
				"- We currently are focusing on ways to make our code look clean by making sure that a simple question can receive an answer. We have removed a lot of unnecessary code that just made what we had look like a lot. We stuck to what just seems to work and now we are currently findings ways to make our output go from seeming like it is a random answer to actually being an answer which makes sense \n"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Proposed Solution\n",
				"\n",
				"Our proposed solution to make sure that the user will get an answer which makes sense by implementing code that notices what the user is asking to give back a specific answer. We currently decided to use if statements. So, if our user starts off by asking a 'can' question it would give back a certain reply. This is how we plan to make sure this project can get interesting. While it may seem basic to be working this, it is so fun to do this. We know that classes can be serious at times but with this project and the liberty we have to do what we want with this is just so awesome and we truly are having a great time working on this. We hope our final product can help others see that this is something cool and can be played around with. But when focusing on how to better this proposed problem of scalability, we so far have come up with data parallelism when working on the code we have so far. What we mean by this is making the data we have into small subsets which then process them. Not only is this a way to clean up our data and make it look nice, but can be a start to combat against scalability issues among algorithms. Another way to combat against this proposed problem is actually to not just use the if statements but instead use dictionary to store possible outcomes for our simulator. We are still expanding these general idea to better improve our idea to create a better ML-solution."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Evaluation Metrics\n",
				"\n",
				"One evaluation metric to be sure our code is working as it should is to definitely place in a numerous amount of questions that are structured in the way to make sure our if statements throughout the code are working properly. After brainstorming more on how we can make this idea more of a ML based one, we decided to implement Recall. We can use recall to interpret a positive sample of a user asking a question that can be answered and if identified it would give back an accurate response. We will add a set of labeled data to train the model to predict whether a question can be answered by the 8 ball, or if it cannot. We plan on further expanding on this idea of course and this is just a general idea we came up with as a team."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Results\n",
				"\n",
				"NEW SECTION!\n",
				"\n",
				"Please show any preliminary results you have managed to obtain.\n",
				"\n",
				"- In the beginning we started off with only a few answers to any given question and as of right now, we have a total of 14 answers. While this is an improvement, we do not consider this a pass because while we do have a good amount of answers, they appear to be random rather than being meanful to any given question. This currently is a struggle for us because we want to make the output not seem random. Or at least that is our current goal. Here is a small part of our code to showcase the output, first we have the answers which we numbered as (Answer 1) and then we can ask any given question and to make it interesting the output first starts off by stating, (Oh hello there, did not expect to see anyone around this time. My name is Steven but most people call me the magical 8 ball haha. Well enter your name, this helps me sink into the many alternate timelines to give you a concrete answer) which then the user can place their name and then it will be prompted with (If you put your name as Jake for example) (Jake, Ahh yes! I knew you would come here. Get it? Sorry my humors all ROUND up. Hahah because I'm a ball. Well anyways, ask away..) That is the current output we have and we want to expand upon this to make it even more unique and of course after a question is asked it will reply with more than just 14 different answers."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Ethics & Privacy"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"We did not explain well enough last time but what we meant to say for this section is that many algorithms/code face issues with bias/discrimination and we are still trying to find ways so that this is avoided.\n",
				"\n",
				"Another concern that happens a lot especially with machine learning is unexpected outcomes. At times this could simply be a wrong answer or a very wrong inappropriate one. We are trying our best so that these issues and others that seem to occur will not be ones we have to deal with. As of right now, our current plan to make sure these are avoided is to make sure we regularly update/look over our data to make sure nothing bad has been placed or something that can be perceived as such."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Team Expectations "
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"* The team expectations above all else is that we will work as equals and no one is better than the other or should feel as they have to do all the work themselves.\n",
				"* Meet weekly on Discord\n",
				"* Constant updates via Discord\n",
				"* Maintain a safe work environment for all groupmates\n",
				"* If a problem occurs, then the one experiencing said problem should feel free to ask the other for help because at the end of the day we are not just a team of random students here at UCSD, we are a team that can work well and efficient to meet the given deadlines given to us by our wonderful professor\n",
				"* We will work on this project when we both have the free time to do so, for example in the afternoons after our core classes are done"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Project Timeline Proposal"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
				"|---|---|---|---|\n",
				"| 2/21  | 7 PM |  Discuss other topics for project if need be  | Communication among team members is key | \n",
				"| 2/22  | 7 PM |  Assign what to do | Finish up project proposal | \n",
				"| 2/25  | 7 PM |  Discuss ways to make the current stage of code better | Give feedback for teammates |\n",
				"| 3/1  | 7 PM  | Go over methods that worked to clean our code (This worked for us to realize we should use and figure out how we can use if statements so our code can flow better)| Discuss next steps for project |\n",
				"| 3/8  | 6 PM  | Look at peer review responses | Discuss/edit project code (This helped us figure out what we really want from our project. While we were given feedback to perhaps think of something else we still want to work on what we think can have great potential in the finalized product, but there is still room to rethink) |\n",
				"| 3/13  | 5 PM  | Draft results/conclusion/discussion | Discuss/edit |\n",
				"| 3/22  | 2 PM  | Final touches to the project| Turn in Final Project  |"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3.7.4 64-bit",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.7.4"
		},
		"vscode": {
			"interpreter": {
				"hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
