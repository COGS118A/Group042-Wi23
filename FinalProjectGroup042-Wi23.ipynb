{
	"cells": [{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# COGS 118A - Project Checkpoint"
			]
		},
		{
				"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Names\n",
				"\n",
				"- Masiah Manzano\n",
				"- Jesus Enrique Mendez\n",
				"- Armaan Johal\n",
				"- \n",
				"- "
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Abstract \n",
				"The goal that we want to solve is to help the world find an answer to whatever it is that they are asking for. We thought back to how when we were young, we could ask a magic 8 ball anything and receive an answer, but what if we can somehow manipulate this so that the magic 8 ball can be more like how CHAT GPT is for everyone nowadays. Where the user will ask a question they feel they need an answer to that is not necessarily academic, and unlike CHAT GPT, it can be about anything with no restrictions and still receive an answer.The data we will be utilizing is yet to be determined but we plan on taking a public poll from our peers on what questions they would really want answered and it can be about anything. Lastly, success will be measured when an adequate answer can be given that means satisfactory for the one who is asking for magic 8 ball. The data used throughout this project represents what the user will be inputting in the format of a question and it will be measured through various responses that are placed into the code to give the user a variety of different answers for what they are asking. Lastly, performance will be measured if the answers given back actually make sense for what is asked, if it is too random then this will be measured as a failure since our goal here is to give back the user a answer they can be happy with. \n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Background\n",
				"\n",
				"What helped us come up with this idea is finding ways to make a lot of input data be used and give back an output. This is important because as we researched online, we found that many machine learning algorithms receive a lot of data and because of this cannot scale as good as it needs to. A good example of this is when algorithms use K-means. A paper we found (https://dl.acm.org/doi/10.1145/2347583.2347596) discusses how K-means is a learning algorithm that implements this and as the data contains to increase, the time it takes to output is excessive. So what if we can use the 8 ball simulator as a way to limit down response times even if the data is huge (this general problem is still likely to change but this is the best we could come up with to make sure we can still use this idea as a ML-learning solution for a ML problem). Another reference that dives deeper into scalability comes from (https://livebook.manning.com/book/real-world-machine-learning/chapter-9/1) Here we see how scalability has been a constant issue for many years now for algorithms and we are trying to find ways to use our idea to combat against this or to showcase how we can better the workload while still using a lot of data sets"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Problem Statement\n",
				"\n",
				"Given the feedback we received, we decided to take a different approach on how we plan to use what we came up with. We still want to use our 8 ball simulator but in a way that can help answer a ML-potential question and while researching certain ways to better this problem, we have come to the conclusion that we should take an approach of either scalability, or to help combat biased algorithms and systems. We believe that our current 8 ball simulator can either focus primarily on both or one of these because, if we take the approach to help scalability since most models become difficult as data is increased, we want to showcase that this is not always the case because we want to be able to generate as many answers as we possibly can and still be able to give an output. Again, this is still a work in progress but the general area we want to focus on is scalability and how we can use this simulation as an example for other models who suffer from too much data, which this can be used as an example how they can better their models to take up a lot of data and still work good."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Data\n",
				"\n",
				"- Since it would take a really long time we decided it is best to not interview others (as fun as it may be) and instead just search on our own time commonly asked questions and gather information all in just one sitting and apply this to our code. We decided to research on some common questions asked by people and we figured the best way to do this is by asking questions that truly get to know someone so we stumbled upon these 100 questions and here is the link https://www.signupgenius.com/groups/getting-to-know-you-questions.cfm. We primarily used this as a reference for the type of questions we want to ask and for what to expect to receive if these types of questions are asked. Another example that we took a look at to help combat scalability issues with our code and to help better other algorithms using this idea of an 8 ball simulator, we took a look at Ray. Ray is a software program that takes heavy workloads of code and makes it really easy to scale the workload. We are going to continue looking at how this program seems to help other algorithms and see if we can apply this or any of the methods used to better our general idea on how to tackle a ML-problem \n",
				"- At the end of this journey we still did not reach the amount of variables we wanted. This is mostly due to the lack of having a full team that others luckily did have\n",
				"- Our observations consist of finding more ways for us to implement methods of greater responses. What we mean by this is that we want to be able to answer questions without a one word answer. We want the user to actually feel as if they are having a conversation with our code\n",
				"- In the end we made sure our code looks clean and did so by making sure that a simple question can receive an answer. We have removed a lot of unnecessary code that just made what we had look like a lot. We stuck to what just seems to work and now we are currently findings ways to make our output go from seeming like it is a random answer to actually being an answer which makes sense \n"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Proposed Solution\n",
				"\n",
				"Our proposed solution to make sure that the user will get an answer which makes sense by implementing code that notices what the user is asking to give back a specific answer. We currently decided to use if statements. So, if our user starts off by asking a 'can' question it would give back a certain reply. This is how we plan to make sure this project can get interesting. While it may seem basic to be working this, it is so fun to do this. We know that classes can be serious at times but with this project and the liberty we have to do what we want with this is just so awesome and we truly are having a great time working on this. We hope our final product can help others see that this is something cool and can be played around with. But when focusing on how to better this proposed problem of scalability, we so far have come up with data parallelism when working on the code we have so far. What we mean by this is making the data we have into small subsets which then process them. Not only is this a way to clean up our data and make it look nice, but can be a start to combat against scalability issues among algorithms. Another way to combat against this proposed problem is actually to not just use the if statements but instead use dictionary to store possible outcomes for our simulator. We are still expanding these general idea to better improve our idea to create a better ML-solution."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Evaluation Metrics\n",
				"\n",
				"One evaluation metric to be sure our code is working as it should is to definitely place in a numerous amount of questions that are structured in the way to make sure our if statements throughout the code are working properly. After brainstorming more on how we can make this idea more of a ML based one, we decided to implement Recall. We can use recall to interpret a positive sample of a user asking a question that can be answered and if identified it would give back an accurate response. We will add a set of labeled data to train the model to predict whether a question can be answered by the 8 ball, or if it cannot. We plan on further expanding on this idea of course and this is just a general idea we came up with as a team."
			]
	},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Results\n",
				"\n",
				"You may have done tons of work on this. Not all of it belongs here. \n",
				"\n",
				"- In the beginning we started off with only a few answers to any given question and as of right now, we have a total of 14 answers. While this is an improvement, we do not consider this a pass because while we do have a good amount of answers, they appear to be random rather than being meanful to any given question. This currently is a struggle for us because we want to make the output not seem random. Or at least that is our current goal. One point we want to highlight is how we structured our code and here is an example of how we did so. 'import random' was used primarily so that it can use one of the various answers that is pre-determined to give to the user when a yes or no question is asked. But what we really want to showcase that can show what was used as an AI aspect is to detect positivity or negativity for questions and we decided to not place the code here but rather attach it as another file in our group042 folder titled Final Project. To analyze text to see if a question can be perceived as positive or negative, we decided to go with textblob and we imported it to our code to see when a user asks questions. And if it detects that the question is positive it will give out a more positive answer out of the random answers we have given to our code. In the end we had a total of twenty seven variables. Other important things we want to highlight is we used the code get sentiment to test if a question is considered negative which you will see in the attached file in our groups repository! Also, it is a continuous chat with our simulator, so when the user runs the code, it will continue on forever and the user can ask as many questions as they desire and if they ever want to leave or stop asking questions, they simply have to type in N and the simulator will end. This is prompted to the user after every question is answered in case they want to leave or continue."
			]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "The main point here is that we got a working conversation that is continuous until the user wants it to stop. We are making this our main point and not the negativity or positive aspect, because in the beginning the conversation was one sided and it ended after just one question. To even get the chance to ask another question, we would have to rerun our code and that was just not efficient enough for us so we decided to loop our code and even made it possible for the user to end this conversation. Although this weems small, we had such a fun time getting this code to work properly and run smoothly. Another thing we want to have noticed is actually our failure and we will also address this in limitation but w e obviously did not reach over 10k observations. We were pretty limited in the amount of hands on deck for this project as some other members did not reach out to us and also finals week was hectic for us all. However we would like to add that we think we did a great job and still managed to have a fun time and got to submit something that we are not only porous of, but something we enjoyed doing and researhcing.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "As mentioned above, we went through the problem of not reaching over 10k observations. And while this is no valid excuse, we still tried our best with the small team we had compared to others. We went through constant problems of trying to get our other team members to not only contribute but to at least say something to contribute as a whole. However, the ones who were able to help did the best they could. We all experienced a hectic few weeks, some went through family issues and even sickness. However, we still stepped up and tried our best to showcase what we were able to do.   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "We did not explain well enough last time but what we meant to say for this section is that many algorithms/code face issues with bias/discrimination and we are still trying to find ways so that this is avoided. Another concern that happens a lot especially with machine learning is unexpected outcomes. At times this could simply be a wrong answer or a very wrong inappropriate one. We are trying our best so that these issues and others that seem to occur will not be ones we have to deal with. As of right now, our current plan to make sure these are avoided is to make sure we regularly update/look over our data to make sure nothing bad has been placed or something that can be perceived as such.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Overall, the main point of our final project was to showcase how our creativity can be implemented with machine learning. We all decided that an 8 ball is something we enjoyed in our youth to ask any questions so we decided to make our own simulation for this 8 ball but now digital. In the end we decided to incorporate what can be seen as positive or what can be seen as negativity and we saw that textblob can be utilized to help us ensure any given text can be analyzed to fall into a category of whether it is positive and give back positive reinforcement to the user. If the user is being negative we decided to give back even more positive reinforcement to the user and not always give an answer but just try to be positive so the user can be happy, if they ask a negative question to our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Arenâ€™t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
